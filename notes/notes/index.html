
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../A%20bit%20of%20probability%20theory/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.3">
    
    
      
        <title>Robust Monte Carlo Methods For Light Transport Simulation - Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.c4a75a56.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#robust-monte-carlo-methods-for-light-transport-simulation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Notes" class="md-header__button md-logo" aria-label="Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Robust Monte Carlo Methods For Light Transport Simulation
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Welcome to MkDocs
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../A%20bit%20of%20probability%20theory/" class="md-tabs__link md-tabs__link--active">
        Notes
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Notes" class="md-nav__button md-logo" aria-label="Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Welcome to MkDocs
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../A%20bit%20of%20probability%20theory/" class="md-nav__link">
        A bit of probability theory
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Robust Monte Carlo Methods For Light Transport Simulation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Robust Monte Carlo Methods For Light Transport Simulation
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#chapter-1-introduction" class="md-nav__link">
    Chapter 1: Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-2-monte-carlo-integration" class="md-nav__link">
    Chapter 2: Monte Carlo Integration
  </a>
  
    <nav class="md-nav" aria-label="Chapter 2: Monte Carlo Integration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#probability-theory" class="md-nav__link">
    Probability theory
  </a>
  
    <nav class="md-nav" aria-label="Probability theory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cdf-and-pdf-functions" class="md-nav__link">
    CDF and PDF functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expected-value-and-variance" class="md-nav__link">
    Expected value and variance
  </a>
  
    <nav class="md-nav" aria-label="Expected value and variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#properties" class="md-nav__link">
    Properties
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conditional-and-marginal-densities" class="md-nav__link">
    Conditional and marginal densities
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basic-monte-carlo-integration" class="md-nav__link">
    Basic Monte Carlo integration
  </a>
  
    <nav class="md-nav" aria-label="Basic Monte Carlo integration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monte-carlo-estimator" class="md-nav__link">
    Monte Carlo estimator
  </a>
  
    <nav class="md-nav" aria-label="Monte Carlo estimator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#properties_1" class="md-nav__link">
    Properties
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unbiased-estimators" class="md-nav__link">
    Unbiased estimators
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance-reduction-analytic-integration" class="md-nav__link">
    Variance reduction: Analytic integration
  </a>
  
    <nav class="md-nav" aria-label="Variance reduction: Analytic integration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-of-expected-values" class="md-nav__link">
    Use of expected values
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#importance-sampling" class="md-nav__link">
    Importance sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#control-variates" class="md-nav__link">
    Control variates
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance-reduction-uniform-sample-placement" class="md-nav__link">
    Variance reduction: Uniform sample placement
  </a>
  
    <nav class="md-nav" aria-label="Variance reduction: Uniform sample placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stratified-sampling" class="md-nav__link">
    Stratified sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latin-hypercube-sampling" class="md-nav__link">
    Latin hypercube sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonal-array-sampling" class="md-nav__link">
    Orthogonal array sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quasi-monte-carlo-methods" class="md-nav__link">
    Quasi-Monte Carlo methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance-reduction-adaptive-sample-placement" class="md-nav__link">
    Variance reduction: Adaptive sample placement
  </a>
  
    <nav class="md-nav" aria-label="Variance reduction: Adaptive sample placement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adaptive-sampling" class="md-nav__link">
    Adaptive sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#russian-roulette-and-splitting" class="md-nav__link">
    Russian roulette and splitting
  </a>
  
    <nav class="md-nav" aria-label="Russian roulette and splitting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#russian-roulette" class="md-nav__link">
    Russian roulette
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#splitting" class="md-nav__link">
    Splitting
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance-reduction-correlated-estimators" class="md-nav__link">
    Variance reduction: Correlated estimators
  </a>
  
    <nav class="md-nav" aria-label="Variance reduction: Correlated estimators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#antithetic-variates" class="md-nav__link">
    Antithetic variates
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-methods" class="md-nav__link">
    Regression methods
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-3-radiometry-and-light-transport" class="md-nav__link">
    Chapter 3: Radiometry and Light Transport
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-4-a-general-operator-formulation-of-light-transport" class="md-nav__link">
    Chapter 4: A General Operator Formulation of Light Transport
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-8-a-path-integral-formulation-of-light-transport" class="md-nav__link">
    Chapter 8: A Path Integral Formulation of Light Transport
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-9-multiple-importance-sampling" class="md-nav__link">
    Chapter 9: Multiple Importance Sampling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-10-bidirectional-path-tracing" class="md-nav__link">
    Chapter 10: Bidirectional Path Tracing
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#chapter-11-metropolis-light-transport" class="md-nav__link">
    Chapter 11: Metropolis Light Transport
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#links" class="md-nav__link">
    Links
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="robust-monte-carlo-methods-for-light-transport-simulation">Robust Monte Carlo Methods For Light Transport Simulation</h1>
<h3 id="chapter-1-introduction">Chapter 1: Introduction</h3>
<p>The focus is set on unbiased, view dependent, Monte Carlo algorithms.</p>
<blockquote>
<p>A view-independent algorithm is an algorithm that computes an intermediate representation of the solution, from which we can create arbitrary views quickly. Any other algorithm is view-dependant.</p>
<p>Put it simply, an unbiased estimator computes the correct answer, on average. A biased estimator computes the wrong answer on average. However, if a biased estimator is consistent, then the average error can be made arbitrarily small by increasing the sample size.</p>
</blockquote>
<p>The rainbow effect created by a light going through a transmission is called a <code>dispersion</code></p>
<h2 id="chapter-2-monte-carlo-integration">Chapter 2: Monte Carlo Integration</h2>
<h3 id="probability-theory">Probability theory</h3>
<h4 id="cdf-and-pdf-functions">CDF and PDF functions</h4>
<p>Cumulative distribution function (cdf) of a real-valued random variable <span class="arithmatex">\(X\)</span> is defined as:</p>
<div class="arithmatex">\[P(x) = P_r \{ X \le x \} = \int_0^x p(X)dX\]</div>
<blockquote>
<p>The cdf is the sum of the probability that an event <span class="arithmatex">\(X\)</span> is less or equal to <span class="arithmatex">\(x\)</span></p>
<p><span class="arithmatex">\(dx\)</span> represent the volume integrated for the integral</p>
<p><span class="arithmatex">\(P(x)\)</span> can also be written as <span class="arithmatex">\(F(x)\)</span></p>
</blockquote>
<p>and the corresponding probability density function (also known as pdf) is:</p>
<div class="arithmatex">\[ p(x) = \frac{dP}{dx}(x)\]</div>
<blockquote>
<p><span class="arithmatex">\(p(x)\)</span> represent the rate of change of the cdf for an event <span class="arithmatex">\(x\)</span>.</p>
<p>For a function <span class="arithmatex">\(f(x) = y\)</span>, if we define <span class="arithmatex">\(F\)</span> as the sum of all the values of <span class="arithmatex">\(f(x)\)</span>, therefore <span class="arithmatex">\(f(x) = y = \frac{dF}{dx}(x)\)</span>.</p>
</blockquote>
<p>This leads to the following relationship:</p>
<div class="arithmatex">\[P_r \{\alpha \le X \le \beta \} = \int_\alpha^\beta p(x)dx = P(\beta) - P(\alpha)\]</div>
<blockquote>
<p>This equation is just an extension of the cdf equation if you think of <span class="arithmatex">\(P(x) = P_r \{ 0 \le X \le x \}\)</span> where <span class="arithmatex">\(\alpha = 0\)</span> and <span class="arithmatex">\(\beta = x\)</span>.</p>
</blockquote>
<p>The corresponding notions for a multidimensional random vector <span class="arithmatex">\((X^1, \dotsc, X^s)\)</span> are the joint cumulative distribution function:</p>
<div class="arithmatex">\[P(x^1, \dotsc, x^s) = P_r \{ X^i \le x^i \text{ for all } i = 1, \dotsc, s \}\]</div>
<p>and the joint density function:</p>
<div class="arithmatex">\[p(x^1, \dotsc, x^s) = \frac{ \partial^s P }{\partial x^1 \dotsb \partial x^s}(x^1, \dotsc, x^s) = \prod_{i = 1}^s \frac{\partial P}{\partial x^i}dx^i\]</div>
<blockquote>
<p>Jacobien</p>
</blockquote>
<p>so that we have the relationship:</p>
<div class="arithmatex">\[P_r \{ x \in D \} = \int_D p(x_1, \dotsc, x_s) dx^1 \dotsb dx^s\]</div>
<p>for any Lebesgue measurable subset <span class="arithmatex">\(D \sub \reals^s\)</span>.</p>
<blockquote>
<p>The 3 equations above are just an evolution of the first ones with <span class="arithmatex">\(x^1, \dotsc, x^s\)</span> instead of <span class="arithmatex">\(X\)</span></p>
</blockquote>
<p>More generally, for a random variable <span class="arithmatex">\(X\)</span> with values in an arbitrary domain <span class="arithmatex">\(\Omega\)</span>, its probability measure (also known as a probability distribution or distribution) is a measure function <span class="arithmatex">\(P\)</span> such that:</p>
<div class="arithmatex">\[P(D) = P_r \{ X \in D \}\]</div>
<p>for any measurable set <span class="arithmatex">\(D \sub \Omega\)</span>. In particular, a probability measure must satisfy <span class="arithmatex">\(P(\Omega) = 1\)</span>.</p>
<p>The corresponding density function <span class="arithmatex">\(p\)</span> is defined as the Radon-Nikodym derivative</p>
<div class="arithmatex">\[p(x) = \frac{dP}{d\mu}(x)\]</div>
<blockquote>
<p><span class="arithmatex">\(d\mu\)</span> replaced <span class="arithmatex">\(dx\)</span></p>
<p><span class="arithmatex">\(\mu\)</span> is a measure of density corresponding on the volume integrated (i.e. area, solid angle, etc...)</p>
</blockquote>
<p>which is simply the function <span class="arithmatex">\(p\)</span> that satisfies</p>
<div class="arithmatex">\[P(D) = \int_D p(x) d\mu(x)\]</div>
<p>The probability that <span class="arithmatex">\(X \in D\)</span> can be obtained by integrating <span class="arithmatex">\(p(x)\)</span> over the given region <span class="arithmatex">\(D\)</span>.</p>
<h4 id="expected-value-and-variance">Expected value and variance</h4>
<p>The expected value or expectation of a random variable <span class="arithmatex">\(Y = f(X)\)</span> is defined as</p>
<div class="arithmatex">\[E[Y] = \int_\Omega f(x)p(x)d\mu(x)\]</div>
<blockquote>
<p>The expected value of <span class="arithmatex">\(Y\)</span> is equal to the sum of all the value of <span class="arithmatex">\(f(x) \in \Omega\)</span> multiplied by its pdf (<span class="arithmatex">\(p(x)\)</span>) and by the derivative of the measure of density (<span class="arithmatex">\(d\mu(x)\)</span>).</p>
<p>Rendering paper tend to simplify this equation by removing <span class="arithmatex">\(p(x)\)</span></p>
</blockquote>
<p>while its variance is</p>
<div class="arithmatex">\[V[Y] = E \left[(Y - E[Y])^2 \right]\]</div>
<p>It is assume that the expected value and variance of every random variable exist (i..e. the corresponding integral is finite).</p>
<blockquote>
<p>The variance of <span class="arithmatex">\(Y\)</span> is equal to the expected value of the square root of, <span class="arithmatex">\(Y\)</span> minus the expected value of <span class="arithmatex">\(Y\)</span>.</p>
</blockquote>
<p>Another useful quantity is the standard deviation of a random variable, which is simply the square root of its variance:</p>
<div class="arithmatex">\[\sigma[Y] = \sqrt{V[Y]}\]</div>
<p>This is also known as the root mean square (RMS) error.</p>
<h5 id="properties">Properties</h5>
<p>For any constant <span class="arithmatex">\(\alpha\)</span>:</p>
<ul>
<li>
<div class="arithmatex">\[E[aY] = aE[Y]\]</div>
</li>
<li>
<div class="arithmatex">\[V[aY] = a^2 V[Y]\]</div>
</li>
</ul>
<p>For any random variables <span class="arithmatex">\(Y_1, \dotsc , Y_N\)</span>:</p>
<ul>
<li>
<div class="arithmatex">\[E \left[ \sum_{i=1}^N Y_i \right] = \sum_{i=1}^N E[Y_i]\]</div>
</li>
</ul>
<p>The following identity holds only if the variables <span class="arithmatex">\(Y_i\)</span> are independent:</p>
<ul>
<li>
<div class="arithmatex">\[V \left[\sum_{i=1}^N Y_i \right] = \sum_{i=1}^N V [Y_i]\]</div>
</li>
</ul>
<blockquote>
<p>Notice that from these rules, we can derive a simpler expression for the variance:</p>
<ul>
<li>
<div class="arithmatex">\[V[Y] = E \left[(Y - E[Y])^2 \right] = E \left[Y^2 \right] - E[Y]^2\]</div>
</li>
</ul>
</blockquote>
<h4 id="conditional-and-marginal-densities">Conditional and marginal densities</h4>
<p>Let <span class="arithmatex">\(X \in \Omega_1\)</span> and <span class="arithmatex">\(Y \in \Omega_2\)</span> be a pair of random variables, so that:</p>
<div class="arithmatex">\[(X, Y) \in \Omega\]</div>
<p>where <span class="arithmatex">\(\Omega = \Omega_1 \times \Omega_2\)</span>.</p>
<p>Let <span class="arithmatex">\(P\)</span> be the joint probability measure of <span class="arithmatex">\((X, Y)\)</span>, so that <span class="arithmatex">\(P(D)\)</span> represents the probability that <span class="arithmatex">\((X, Y) \in D\)</span> for any measurable subset <span class="arithmatex">\(D \sub \Omega\)</span>. Then the corresponding joint density function <span class="arithmatex">\(p(x, y)\)</span> satisfies</p>
<div class="arithmatex">\[P(D) = \int_D p(x, y) d\mu_1(x) d\mu_2(y)\]</div>
<p>where <span class="arithmatex">\(\mu_1\)</span> and <span class="arithmatex">\(\mu_2\)</span> are measures on <span class="arithmatex">\(\Omega_1\)</span> and <span class="arithmatex">\(\Omega_2\)</span> respectively.</p>
<p>The marginal density function of X is now defined as</p>
<div class="arithmatex">\[p(x) = \int_{\Omega_2} p(x, y)dy\]</div>
<blockquote>
<p>Reminder that <span class="arithmatex">\(dy\)</span> is the simplified notation of the measure function notation <span class="arithmatex">\(d\mu(y)\)</span></p>
</blockquote>
<p>while the conditional density function <span class="arithmatex">\(p(x|y)\)</span> is defined as</p>
<div class="arithmatex">\[p(x|y) = \frac{p(x, y)}{p(x)}\]</div>
<p>The marginal density <span class="arithmatex">\(p(y)\)</span> and conditional density <span class="arithmatex">\(p(x | y)\)</span> are defined in a similar way, leading to the useful identity:</p>
<div class="arithmatex">\[p(x, y) = p(y | x) p(x) = p(x | y) p(y)\]</div>
<p>Another important concept is the conditional expectation of a random variable <span class="arithmatex">\(G = g(X, Y)\)</span>, defined as</p>
<div class="arithmatex">\[E \left[ G | x \right] \int_{\Omega_2} g(x, y) p(y|x) dy = \frac{\int g(x, y) p(x, y) dy}{\int p(x, y) dy}\]</div>
<blockquote>
<p><span class="arithmatex">\(E_Y[G]\)</span> is used for conditional expectation, to emphasizes the fact that Y is the random variable whole density function is being integrated.</p>
</blockquote>
<p>There is a very useful expression for the variance of G in terms of its conditional expectation and variance, namely</p>
<div class="arithmatex">\[V[G] = E_X V_Y G + V_X E_Y G\]</div>
<blockquote>
<p><span class="arithmatex">\(V[G]\)</span> is the mean of the conditional variance, plus the variance of the conditional mean</p>
</blockquote>
<h3 id="basic-monte-carlo-integration">Basic Monte Carlo integration</h3>
<p>The idea of Monte Carlo integration is to evaluate the integral:</p>
<div class="arithmatex">\[I = \int_\Omega f(x) d\mu(x)\]</div>
<p>using random sampling. It is done by independently sampling <span class="arithmatex">\(N\)</span> points <span class="arithmatex">\(X^1, \dotsc, X^s\)</span> according to a density function <span class="arithmatex">\(p\)</span>, and then computing the estimate</p>
<div class="arithmatex">\[F_N = \frac{1}{N} \sum_{i = 1}^{N} \frac{f(X_i)}{p(X_i)}\]</div>
<blockquote>
<p>It used <span class="arithmatex">\(F_N\)</span> rather than <span class="arithmatex">\(I\)</span> to emphasize that the result is a random variable</p>
<p>Monte Carlo integration converges at a rate of <span class="arithmatex">\(O(N^{-1/2})\)</span> in any dimension</p>
</blockquote>
<p>For further details:</p>
<ul>
<li>2.4.1 convergence rates: proof of the convergence rates</li>
</ul>
<h4 id="monte-carlo-estimator">Monte Carlo estimator</h4>
<p>We define <span class="arithmatex">\(Q\)</span> (called estimand), as the value of a given integral.</p>
<h5 id="properties_1">Properties</h5>
<ul>
<li>The quantity <span class="arithmatex">\(F_N - Q\)</span> is called the <code>error</code>, and its expected value is called the <code>bias</code>:</li>
</ul>
<div class="arithmatex">\[\beta[F_N] = E[F_N - Q]\]</div>
<ul>
<li>an estimator is called <code>unbiased</code> if <span class="arithmatex">\(\beta[F_N] = 0\)</span> for all sample sizes <span class="arithmatex">\(N\)</span> which means</li>
</ul>
<div class="arithmatex">\[E[F_N] = Q \text{ for all } N \ge 1\]</div>
<ul>
<li>an estimator is called <code>consistent</code> if the error <span class="arithmatex">\(F_N - Q\)</span> goes to zero with probability one</li>
</ul>
<div class="arithmatex">\[P_r \left\{ \lim_{ N \to \infty} F_N = Q \right\} = 1\]</div>
<ul>
<li>another way to say it, it that for an estimator to be consistent, the bias and variance both have to go to zero as <span class="arithmatex">\(N\)</span> is increased</li>
</ul>
<div class="arithmatex">\[\lim_{N \to \infty} \beta[F_N] = \lim_{N \to \infty} V[F_N] = 0\]</div>
<ul>
<li>an unbiased estimator is consistent as long as its variance decrease to zero as <span class="arithmatex">\(N\)</span> goes to infinity</li>
</ul>
<blockquote>
<p>To not confuse with the fact of being unbiased which means that the result of the integral does not have bias for all samples <span class="arithmatex">\(N\)</span>.</p>
</blockquote>
<h5 id="unbiased-estimators">Unbiased estimators</h5>
<p>Typically the goal is to minimize the mean squared error (<span class="arithmatex">\(MSE\)</span>), defined by</p>
<div class="arithmatex">\[MSE[F] = E[(F - Q)^2]\]</div>
<p>Which can be simplified in (Veach p.44 for how to simplify)</p>
<div class="arithmatex">\[MSE[F] = V[F] + \beta[F]^2\]</div>
<p>For unbiased estimators, we can simplified it even further since <span class="arithmatex">\(\beta[F] = 0\)</span></p>
<div class="arithmatex">\[MSE[F] = V[F] = E[(F - E[F])^2]\]</div>
<p><span class="arithmatex">\(\beta\)</span> is non-trivial to compute, which make computing the error difficult for biased estimator. Thus, error estimates are easier to obtain for unbiased estimators. </p>
<p>Letting <span class="arithmatex">\(Y_1, \dotsc, Y_N\)</span> be independent samples of an unbiased estimator <span class="arithmatex">\(Y\)</span>, and letting</p>
<div class="arithmatex">\[F_N = \frac{1}{N}\sum_{i = 1}^N Y_i\]</div>
<p>as before, then the quantity is</p>
<div class="arithmatex">\[\hat{V}[F_N] = \frac{1}{N-1} \left\{ \left( \frac{1}{N} \sum_{i = 1}^N Y_i^2 \right) - \left( \frac{1}{N} \sum_{i = 1}^N Y_i \right)^2 \right\}\]</div>
<blockquote>
<p>Look for the simpler version of the variance</p>
<p>The hat signal that it is relative to an estimator</p>
</blockquote>
<p>The error of an unbiased estimator can be made as small as desired since</p>
<div class="arithmatex">\[V[F_N] = V[F_1] / N\]</div>
<p>The goal is to find an estimator whose variance and running time are both small. This is describe by the efficiency of a Monte Carlo estimator:</p>
<div class="arithmatex">\[\epsilon[F] = \frac{1}{V[F]T[F]}\]</div>
<p>where <span class="arithmatex">\(T[F]\)</span> is the time required to evaluate <span class="arithmatex">\(F\)</span>.</p>
<h3 id="variance-reduction-analytic-integration">Variance reduction: Analytic integration</h3>
<p>The techniques developed to design efficient estimators are often called <code>variance reduction methods</code>. These methods are usually grouped into categories base around 4 main ideas:</p>
<ul>
<li>
<p><code>analytically</code> integrating a function that is similar to the integrand</p>
</li>
<li>
<p><code>uniformly</code> placing sample points across the integration domain</p>
</li>
<li>
<p><code>adaptively</code> controlling the sample density based on information gathered during sampling</p>
</li>
<li>
<p>combining samples from two or more estimators whose values are correlated</p>
</li>
</ul>
<h4 id="use-of-expected-values">Use of expected values</h4>
<p>One way to reduce variance is to reduce the dimension of the sample space, by integrating analytically with respect to one or more variables of the domain. It consists of replacing an estimator of the form</p>
<div class="arithmatex">\[F = f(X, Y) / p(X, Y)\]</div>
<p>with one of the form</p>
<div class="arithmatex">\[F' = f'(X) / p(X)\]</div>
<p>where <span class="arithmatex">\(f'(x)\)</span> and <span class="arithmatex">\(p(x)\)</span> are defined by</p>
<div class="arithmatex">\[f'(x) = \int f(x, y) dy\]</div>
<div class="arithmatex">\[p(x) = \int p(x, y) dy\]</div>
<blockquote>
<p>This formulas explain the relationship between <span class="arithmatex">\(f'(x)\)</span>, <span class="arithmatex">\(p(x)\)</span> and respectively <span class="arithmatex">\(f(x, y)\)</span>, <span class="arithmatex">\(p(x, y)\)</span>.</p>
</blockquote>
<p>Thus, to apply this technique, we must be able to integrator both <span class="arithmatex">\(\int\)</span> and <span class="arithmatex">\(p\)</span> with respect to y.</p>
<blockquote>
<p>The use of expected values is the preferred variance reduction technique, as long as it is not too expensive to evaluate and sample the analytically integrated quantities.</p>
<p>Note that if expected values are used for only one part of a larger calculation, then variance can increase.</p>
</blockquote>
<h4 id="importance-sampling">Importance sampling</h4>
<p>Importance sampling refers to the principle of choosing a density function <span class="arithmatex">\(p\)</span> that is similar to the integrand <span class="arithmatex">\(\int\)</span>.</p>
<p>The best choice is to let <span class="arithmatex">\(p(x) = cf(x)\)</span>, where the constant of proportionality, to ensure <span class="arithmatex">\(p\)</span> integrates to one, is</p>
<div class="arithmatex">\[c = \frac{1}{\int_{\Omega}f(y)d\mu(y)}\]</div>
<p>This leads to an estimator with zero variance, since</p>
<div class="arithmatex">\[F = \frac{f(X)}{p(X)} = \frac{f(X)}{cf{X}} = \frac{1}{c}\]</div>
<p>for all sample points <span class="arithmatex">\(X\)</span>.</p>
<blockquote>
<p>Not practical since we must already know the value of the desired integral to compute the constant</p>
<p>Importance sampling is one of the most useful and powerful techniques of Monte Carlo integration. </p>
</blockquote>
<h4 id="control-variates">Control variates</h4>
<p>The idea behind control variates, is to find a function <span class="arithmatex">\(g\)</span> that can be integrated analytically and is similar to the integrand, and then subtract it. The integral is rewritten as</p>
<div class="arithmatex">\[I = \int_{\Omega} g(x) d\mu(x) + \int_{\Omega} f(x) - g(x) d\mu(x)\]</div>
<p>and then sampled with an estimator of the form</p>
<div class="arithmatex">\[F = \int_{\Omega} g(x) d\mu(x) + \frac{1}{N} \sum_{i = 1}^{N} \frac{f(X_i - g(X_i))}{p(X_i)}\]</div>
<p>where the value of the first integral is known exactly.</p>
<p>The estimator will have a lower variance than the basic estimator whenever</p>
<div class="arithmatex">\[V\left[ \frac{f(X_i) - g(X_i)}{p(X_i)} \right] \le V \left[ \frac{f(X_i)}{p(X_i)} \right]\]</div>
<p>Given a function <span class="arithmatex">\(g\)</span> that is an approximation to <span class="arithmatex">\(f\)</span>, we must decide whether to use it as a control variate or as a density function for importance sampling.</p>
<blockquote>
<p>It is possible that either one of the choice could be the best</p>
</blockquote>
<p>In general, if:</p>
<ul>
<li>
<p><span class="arithmatex">\(f - g\)</span> is nearly a constant function, then <span class="arithmatex">\(g\)</span> should be used as a control variate</p>
</li>
<li>
<p><span class="arithmatex">\(f/g\)</span> is nearly a constant, then <span class="arithmatex">\(g\)</span> should be used for importance sampling</p>
</li>
</ul>
<blockquote>
<p>Control variates have very few application in graphics currently. This technique can produce negative sample values, even for an integrand that is strictly positive. This can lead to large relative errors for integrals whose true value is close to zero.</p>
<p>On the other hand, the method is straightforward to apply, and can potentially give a modest variance reduction at little cost.</p>
</blockquote>
<h3 id="variance-reduction-uniform-sample-placement">Variance reduction: Uniform sample placement</h3>
<p>Another important strategy for reducing variance is to ensure that samples are distributed more or less uniformly over the domain.</p>
<p>This technique usually assumed that the domain is the <span class="arithmatex">\(s\)</span>-dimensional unit cube <span class="arithmatex">\([0, 1]^s\)</span>. The other domain can be handled by defining the appropriate transformation of the form <span class="arithmatex">\(T: [0, 1]^s \rightarrow \Omega\)</span>.</p>
<blockquote>
<p>Note that by choosing different mapping <span class="arithmatex">\(T\)</span>, the transformed samples can be given different density functions</p>
</blockquote>
<h4 id="stratified-sampling">Stratified sampling</h4>
<p>The idea of stratified sampling is to subdivide the domain <span class="arithmatex">\(\Omega\)</span> into several non-overlapping regions <span class="arithmatex">\(\Omega_1, \dotsc, \Omega_m\)</span> such that</p>
<div class="arithmatex">\[\bigcup_{i = 1}^n \Omega_i = \Omega\]</div>
<blockquote>
<p>The union of all the regions <span class="arithmatex">\(\Omega_i\)</span> from 0 to <span class="arithmatex">\(n\)</span> is equal to the domain <span class="arithmatex">\(\Omega\)</span> </p>
</blockquote>
<p>Each region <span class="arithmatex">\(\Omega_i\)</span> is called a stratum. A fixed number of samples <span class="arithmatex">\(n_i\)</span>, is taken within each <span class="arithmatex">\(\Omega_i\)</span>, according to some given density function <span class="arithmatex">\(p\)</span>.</p>
<p>Stratified sampling is a useful, inexpensive variance reduction technique. It is mainly effective for low dimensional integration problems where the integrand is reasonably well-behaved.</p>
<h4 id="latin-hypercube-sampling">Latin hypercube sampling</h4>
<p>The idea of latin hypercube sampling is to subdivide the domain <span class="arithmatex">\([0, 1]^s\)</span> into <span class="arithmatex">\(N\)</span> sub-intervals along each dimension, and to ensure that one sample lies in each sub-interval.</p>
<div class="arithmatex">\[X_i^j = \frac{\pi_j(i) - U_{i, j}}{N}\]</div>
<ul>
<li>
<p><span class="arithmatex">\(X_i^j\)</span> denotes the <span class="arithmatex">\(j\)</span>-th coordinate of the sample <span class="arithmatex">\(X_i\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(U_{i, j}\)</span> are independent and uniformly distributed on <span class="arithmatex">\([0, 1]\)</span></p>
</li>
</ul>
<p>In two dimensions, the sample pattern corresponds to the occurrences of a single symbol in a Latin square (i.e an <span class="arithmatex">\(N \times N\)</span> array of <span class="arithmatex">\(N\)</span> symbols such that no symbol appears twice in the same row or column).</p>
<h4 id="orthogonal-array-sampling">Orthogonal array sampling</h4>
<h4 id="quasi-monte-carlo-methods">Quasi-Monte Carlo methods</h4>
<h3 id="variance-reduction-adaptive-sample-placement">Variance reduction: Adaptive sample placement</h3>
<h4 id="adaptive-sampling">Adaptive sampling</h4>
<p>do some research</p>
<h4 id="russian-roulette-and-splitting">Russian roulette and splitting</h4>
<p>Their purpose is to decrease the sample density where the integrand is small and increase it where the integrand is large. These technique do not introduce bias.</p>
<h5 id="russian-roulette">Russian roulette</h5>
<p>It is applied to estimators that are the sum of many terms: <span class="arithmatex">\(F = F_1 + \dotsc + F_N\)</span>. This technique solve the issue where the contributions are very small, and yet as expensive to evaluate as the others contributions. The idea of russian roulette is to randomly skip most of the evaluations associated with small contributions, by replacing these <span class="arithmatex">\(F_i\)</span> with new estimators of the form</p>
<div class="arithmatex">\[F_i' = \begin{cases} \frac{1}{q_i}F_i &amp; \quad \text{with probability } q_i \\ 0 &amp; \quad \text{otherwise} \end{cases}\]</div>
<p>The evaluation probability <span class="arithmatex">\(q_i\)</span> is chosen for each <span class="arithmatex">\(F_i\)</span> separately, based on some convenient estimate of its contribution.</p>
<blockquote>
<p>The estimator <span class="arithmatex">\(F_i'\)</span> is unbiased whenever <span class="arithmatex">\(F_i\)</span>.</p>
<p>This technique increases variance. Russian roulette can still increase efficiency by reducing the average time required to evaluate <span class="arithmatex">\(F\)</span>.</p>
</blockquote>
<p>Suppose that each <span class="arithmatex">\(F_i\)</span> represents the contribution of a particular light source to the radiance reflected from a surface. To reduce the number of visibility tests using Russian roulette, we first compute a tentative contribution <span class="arithmatex">\(t_i\)</span> for each <span class="arithmatex">\(F_i\)</span> ignoring blockers. The a fixed threshold <span class="arithmatex">\(\delta\)</span> is typically chosen, and the probabilities <span class="arithmatex">\(q_i\)</span> are set to</p>
<div class="arithmatex">\[q_i = \min(1, t_i / \delta)\]</div>
<blockquote>
<p>Thus contribution larger than <span class="arithmatex">\(\delta\)</span> are always evaluated, while smaller contribution are randomly skipped in a way that does not cause bias.</p>
</blockquote>
<h5 id="splitting">Splitting</h5>
<p>Russian roulette is closely related to splitting, a technique in which an estimator <span class="arithmatex">\(F_i\)</span> is replaced by one of the form</p>
<div class="arithmatex">\[F_i' = \frac{1}{k} \sum_{j = 1}^k F_{i, j}\]</div>
<p>where the <span class="arithmatex">\(F_{i, j}\)</span> are independent samples from <span class="arithmatex">\(F\)</span>. </p>
<h3 id="variance-reduction-correlated-estimators">Variance reduction: Correlated estimators</h3>
<h4 id="antithetic-variates">Antithetic variates</h4>
<h4 id="regression-methods">Regression methods</h4>
<h2 id="chapter-3-radiometry-and-light-transport">Chapter 3: Radiometry and Light Transport</h2>
<p>manifold?</p>
<p>phase space?</p>
<p>trajectory space?</p>
<p>| Term | Description |</p>
<p>|:-:|:-:|</p>
<p>| <span class="arithmatex">\(\mathcal{M}\)</span> | union of a finite set of surfaces in <span class="arithmatex">\(\reals^3\)</span> (scene geometry)|</p>
<p>| <span class="arithmatex">\(\sigma\)</span> | solid angle |</p>
<p>| <span class="arithmatex">\(\sigma^\perp_x\)</span>| projected solid angle measure on surface x |</p>
<h2 id="chapter-4-a-general-operator-formulation-of-light-transport">Chapter 4: A General Operator Formulation of Light Transport</h2>
<h2 id="chapter-8-a-path-integral-formulation-of-light-transport">Chapter 8: A Path Integral Formulation of Light Transport</h2>
<h2 id="chapter-9-multiple-importance-sampling">Chapter 9: Multiple Importance Sampling</h2>
<h2 id="chapter-10-bidirectional-path-tracing">Chapter 10: Bidirectional Path Tracing</h2>
<h2 id="chapter-11-metropolis-light-transport">Chapter 11: Metropolis Light Transport</h2>
<h2 id="links">Links</h2>
<ul>
<li>
<p><a href="https://graphics.stanford.edu/papers/veach_thesis/thesis.pdf">Robust Monte Carlo Methods For Light Transport Simulation</a>, Eric Veach</p>
</li>
<li>
<p><a href="https://artowen.su.domains/mc/">Monte Carlo theory, methods and examples</a>, Art Owen</p>
</li>
</ul>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.efa0ade1.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/2.3.10/seedrandom.min.js"></script>
      
        <script src="../../js/plot.js"></script>
      
        <script src="../../js/bitOfProbaTheory.js"></script>
      
    
  </body>
</html>